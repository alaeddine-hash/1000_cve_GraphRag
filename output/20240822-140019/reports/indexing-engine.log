14:00:19,307 graphrag.index.cli INFO Logging enabled at output\20240822-140019\reports\indexing-engine.log
14:00:19,321 graphrag.index.cli INFO Starting pipeline run for: 20240822-140019, dryrun=False
14:00:19,323 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": ".",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
14:00:19,362 graphrag.index.create_pipeline_config INFO skipping workflows 
14:00:19,362 graphrag.index.run INFO Running pipeline
14:00:19,363 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at output\20240822-140019\artifacts
14:00:19,365 graphrag.index.input.load_input INFO loading input from root_dir=input
14:00:19,365 graphrag.index.input.load_input INFO using file storage for input
14:00:19,371 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
14:00:19,373 graphrag.index.input.text INFO found text files from input, found [('1000CVE.txt', {})]
14:00:19,378 graphrag.index.input.text INFO Found 1 files, loading 1
14:00:19,382 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
14:00:19,383 graphrag.index.run INFO Final # of rows loaded: 1
14:00:19,577 graphrag.index.run INFO Running workflow: create_base_text_units...
14:00:19,578 graphrag.index.run INFO dependencies for create_base_text_units: []
14:00:19,588 datashaper.workflow.workflow INFO executing verb orderby
14:00:19,596 datashaper.workflow.workflow INFO executing verb zip
14:00:19,605 datashaper.workflow.workflow INFO executing verb aggregate_override
14:00:19,617 datashaper.workflow.workflow INFO executing verb chunk
14:00:20,575 datashaper.workflow.workflow INFO executing verb select
14:00:20,594 datashaper.workflow.workflow INFO executing verb unroll
14:00:20,610 datashaper.workflow.workflow INFO executing verb rename
14:00:20,624 datashaper.workflow.workflow INFO executing verb genid
14:00:20,649 datashaper.workflow.workflow INFO executing verb unzip
14:00:20,665 datashaper.workflow.workflow INFO executing verb copy
14:00:21,47 datashaper.workflow.workflow INFO executing verb filter
14:00:21,134 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
14:00:21,467 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
14:00:21,468 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
14:00:21,469 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
14:00:21,545 datashaper.workflow.workflow INFO executing verb entity_extract
14:00:21,592 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
14:00:21,620 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=0, RPM=0
14:00:21,620 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
14:00:25,654 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:25,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.7030000000013388. input_tokens=2937, output_tokens=92
14:00:26,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:26,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.375. input_tokens=2935, output_tokens=239
14:00:27,891 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:27,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.0. input_tokens=2936, output_tokens=143
14:00:28,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:28,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.218999999999141. input_tokens=2935, output_tokens=261
14:00:28,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:28,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.359999999998763. input_tokens=2936, output_tokens=168
14:00:29,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:29,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.171999999998661. input_tokens=2936, output_tokens=269
14:00:29,185 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:29,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.234000000000378. input_tokens=2936, output_tokens=226
14:00:29,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:29,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.485000000000582. input_tokens=2936, output_tokens=374
14:00:29,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:29,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.781999999999243. input_tokens=2936, output_tokens=244
14:00:29,488 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:29,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.57799999999952. input_tokens=2935, output_tokens=325
14:00:29,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:29,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.609000000000378. input_tokens=2936, output_tokens=466
14:00:30,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.593999999999141. input_tokens=2936, output_tokens=240
14:00:30,424 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.656000000000859. input_tokens=2935, output_tokens=611
14:00:30,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.625. input_tokens=2936, output_tokens=248
14:00:30,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.890999999999622. input_tokens=2937, output_tokens=235
14:00:30,791 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.938000000000102. input_tokens=2937, output_tokens=369
14:00:30,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.015999999999622. input_tokens=2936, output_tokens=384
14:00:30,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:30,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.171999999998661. input_tokens=2935, output_tokens=503
14:00:31,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:31,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.54700000000048. input_tokens=2937, output_tokens=497
14:00:31,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:31,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.656999999999243. input_tokens=2936, output_tokens=552
14:00:31,407 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:31,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.59400000000096. input_tokens=2936, output_tokens=330
14:00:31,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:31,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.375. input_tokens=2936, output_tokens=401
14:00:31,759 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:31,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.563000000000102. input_tokens=2936, output_tokens=137
14:00:32,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.936999999999898. input_tokens=2937, output_tokens=243
14:00:32,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.718999999999141. input_tokens=2935, output_tokens=437
14:00:32,668 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.781000000000859. input_tokens=2937, output_tokens=499
14:00:32,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.343999999999141. input_tokens=2935, output_tokens=347
14:00:32,869 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.875. input_tokens=2936, output_tokens=235
14:00:32,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.04700000000048. input_tokens=2935, output_tokens=251
14:00:32,967 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:32,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.42200000000048. input_tokens=2935, output_tokens=260
14:00:33,151 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:33,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.389999999999418. input_tokens=2936, output_tokens=581
14:00:33,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:33,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.67200000000048. input_tokens=1985, output_tokens=200
14:00:33,766 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:33,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.875. input_tokens=2936, output_tokens=366
14:00:33,884 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:33,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.42200000000048. input_tokens=2935, output_tokens=305
14:00:33,921 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:33,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.843000000000757. input_tokens=2936, output_tokens=440
14:00:34,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:34,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.718999999999141. input_tokens=2936, output_tokens=402
14:00:34,396 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:34,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.139999999999418. input_tokens=2936, output_tokens=364
14:00:34,722 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:34,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.3430000000007567. input_tokens=34, output_tokens=167
14:00:36,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:36,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.703000000001339. input_tokens=34, output_tokens=310
14:00:36,208 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:36,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5319999999992433. input_tokens=34, output_tokens=265
14:00:36,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:36,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.531000000000859. input_tokens=34, output_tokens=353
14:00:36,562 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:36,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0939999999991414. input_tokens=34, output_tokens=247
14:00:36,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:36,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.343999999999141. input_tokens=2936, output_tokens=456
14:00:36,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:36,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.375. input_tokens=34, output_tokens=310
14:00:37,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:37,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.061999999999898. input_tokens=34, output_tokens=485
14:00:37,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:37,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.5. input_tokens=34, output_tokens=261
14:00:37,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:37,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.7190000000009604. input_tokens=34, output_tokens=315
14:00:37,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:37,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.57799999999952. input_tokens=34, output_tokens=363
14:00:37,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:37,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.359999999998763. input_tokens=34, output_tokens=387
14:00:37,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:37,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.186999999999898. input_tokens=34, output_tokens=497
14:00:38,1 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:38,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.860000000000582. input_tokens=34, output_tokens=195
14:00:38,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:38,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.875. input_tokens=34, output_tokens=327
14:00:38,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:38,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.42200000000048. input_tokens=2937, output_tokens=523
14:00:38,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:38,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.04700000000048. input_tokens=34, output_tokens=305
14:00:38,686 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:38,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.968999999999141. input_tokens=2936, output_tokens=580
14:00:38,905 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:38,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.188000000000102. input_tokens=34, output_tokens=311
14:00:40,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:40,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.438000000000102. input_tokens=34, output_tokens=574
14:00:40,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:40,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.5. input_tokens=34, output_tokens=301
14:00:40,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:40,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:40,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.188000000000102. input_tokens=34, output_tokens=275
14:00:40,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.813000000000102. input_tokens=34, output_tokens=598
14:00:40,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:40,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.265999999999622. input_tokens=2935, output_tokens=611
14:00:40,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:40,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.6409999999996217. input_tokens=34, output_tokens=299
14:00:41,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.985000000000582. input_tokens=34, output_tokens=387
14:00:41,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.484000000000378. input_tokens=34, output_tokens=622
14:00:41,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.641000000001441. input_tokens=34, output_tokens=435
14:00:41,539 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.4530000000013388. input_tokens=34, output_tokens=288
14:00:41,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.734000000000378. input_tokens=34, output_tokens=352
14:00:41,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.343000000000757. input_tokens=34, output_tokens=276
14:00:41,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:41,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.485000000000582. input_tokens=34, output_tokens=372
14:00:42,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:42,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.515999999999622. input_tokens=34, output_tokens=861
14:00:42,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:42,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.03099999999904. input_tokens=34, output_tokens=415
14:00:42,685 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:42,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.25. input_tokens=34, output_tokens=357
14:00:43,172 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:43,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.264999999999418. input_tokens=34, output_tokens=342
14:00:43,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:43,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.67200000000048. input_tokens=34, output_tokens=262
14:00:43,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:43,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.796999999998661. input_tokens=34, output_tokens=1138
14:00:44,318 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:44,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.311999999999898. input_tokens=34, output_tokens=458
14:00:44,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:44,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.703000000001339. input_tokens=34, output_tokens=383
14:00:45,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:45,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.5. input_tokens=34, output_tokens=351
14:00:45,562 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:45,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.875. input_tokens=34, output_tokens=435
14:00:46,897 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:46,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.32799999999952. input_tokens=34, output_tokens=798
14:00:48,560 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:00:48,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.28099999999904. input_tokens=34, output_tokens=456
14:01:05,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:05,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.390000000001237. input_tokens=34, output_tokens=267
14:01:06,19 datashaper.workflow.workflow INFO executing verb merge_graphs
14:01:06,157 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
14:01:06,435 graphrag.index.run INFO Running workflow: create_summarized_entities...
14:01:06,435 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
14:01:06,436 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
14:01:06,552 datashaper.workflow.workflow INFO executing verb summarize_descriptions
14:01:08,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:08,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2029999999995198. input_tokens=172, output_tokens=45
14:01:08,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:08,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6869999999998981. input_tokens=170, output_tokens=55
14:01:09,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8130000000001019. input_tokens=268, output_tokens=84
14:01:09,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9369999999998981. input_tokens=187, output_tokens=63
14:01:09,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.04700000000048. input_tokens=171, output_tokens=45
14:01:09,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9220000000004802. input_tokens=181, output_tokens=80
14:01:09,382 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.110000000000582. input_tokens=203, output_tokens=78
14:01:09,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9210000000002765. input_tokens=150, output_tokens=30
14:01:09,490 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0940000000009604. input_tokens=192, output_tokens=43
14:01:09,524 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0309999999990396. input_tokens=187, output_tokens=60
14:01:09,553 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,556 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2340000000003783. input_tokens=161, output_tokens=63
14:01:09,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1090000000003783. input_tokens=146, output_tokens=52
14:01:09,566 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1559999999990396. input_tokens=184, output_tokens=57
14:01:09,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2660000000014406. input_tokens=170, output_tokens=59
14:01:09,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.45299999999952. input_tokens=174, output_tokens=54
14:01:09,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3439999999991414. input_tokens=158, output_tokens=53
14:01:09,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,859 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.57799999999952. input_tokens=176, output_tokens=61
14:01:09,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3590000000003783. input_tokens=172, output_tokens=53
14:01:09,891 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:09,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5939999999991414. input_tokens=268, output_tokens=109
14:01:09,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4839999999985594. input_tokens=180, output_tokens=78
14:01:10,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999998981. input_tokens=180, output_tokens=45
14:01:10,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=277, output_tokens=109
14:01:10,284 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.92200000000048. input_tokens=174, output_tokens=56
14:01:10,329 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9689999999991414. input_tokens=183, output_tokens=40
14:01:10,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,566 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=174, output_tokens=62
14:01:10,590 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.625. input_tokens=190, output_tokens=62
14:01:10,653 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2970000000004802. input_tokens=180, output_tokens=61
14:01:10,658 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000014406. input_tokens=175, output_tokens=59
14:01:10,687 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3590000000003783. input_tokens=190, output_tokens=74
14:01:10,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3430000000007567. input_tokens=176, output_tokens=70
14:01:10,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999991414. input_tokens=166, output_tokens=59
14:01:10,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4380000000001019. input_tokens=226, output_tokens=85
14:01:10,902 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:10,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5. input_tokens=178, output_tokens=73
14:01:11,230 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:11,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0. input_tokens=191, output_tokens=105
14:01:11,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:11,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9219999999986612. input_tokens=642, output_tokens=153
14:01:11,377 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
14:01:11,793 graphrag.index.run INFO Running workflow: create_base_entity_graph...
14:01:11,794 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
14:01:11,795 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
14:01:11,877 datashaper.workflow.workflow INFO executing verb cluster_graph
14:01:12,200 datashaper.workflow.workflow INFO executing verb select
14:01:12,206 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
14:01:12,469 graphrag.index.run INFO Running workflow: create_final_entities...
14:01:12,481 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
14:01:12,481 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
14:01:12,548 datashaper.workflow.workflow INFO executing verb unpack_graph
14:01:12,664 datashaper.workflow.workflow INFO executing verb rename
14:01:12,681 datashaper.workflow.workflow INFO executing verb select
14:01:12,708 datashaper.workflow.workflow INFO executing verb dedupe
14:01:12,747 datashaper.workflow.workflow INFO executing verb rename
14:01:12,794 datashaper.workflow.workflow INFO executing verb filter
14:01:12,917 datashaper.workflow.workflow INFO executing verb text_split
14:01:12,962 datashaper.workflow.workflow INFO executing verb drop
14:01:12,992 datashaper.workflow.workflow INFO executing verb merge
14:01:13,213 datashaper.workflow.workflow INFO executing verb text_embed
14:01:13,215 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
14:01:13,242 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
14:01:13,243 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
14:01:13,286 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 275 inputs via 275 snippets using 18 batches. max_batch_size=16, max_tokens=8191
14:01:14,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,180 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,206 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,380 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3289999999997235. input_tokens=400, output_tokens=0
14:01:14,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.375. input_tokens=676, output_tokens=0
14:01:14,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4529999999995198. input_tokens=386, output_tokens=0
14:01:14,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4840000000003783. input_tokens=483, output_tokens=0
14:01:14,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,966 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:14,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6869999999998981. input_tokens=622, output_tokens=0
14:01:15,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.735000000000582. input_tokens=619, output_tokens=0
14:01:15,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.75. input_tokens=578, output_tokens=0
14:01:15,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8130000000001019. input_tokens=610, output_tokens=0
14:01:15,202 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,203 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.875. input_tokens=550, output_tokens=0
14:01:15,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9380000000001019. input_tokens=56, output_tokens=0
14:01:15,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.07799999999952. input_tokens=515, output_tokens=0
14:01:15,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.2039999999997235. input_tokens=684, output_tokens=0
14:01:15,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.234999999998763. input_tokens=571, output_tokens=0
14:01:15,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3289999999997235. input_tokens=677, output_tokens=0
14:01:15,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.45299999999952. input_tokens=555, output_tokens=0
14:01:15,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.5310000000008586. input_tokens=650, output_tokens=0
14:01:15,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:01:15,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.6090000000003783. input_tokens=454, output_tokens=0
14:01:16,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.82799999999952. input_tokens=654, output_tokens=0
14:01:16,267 datashaper.workflow.workflow INFO executing verb drop
14:01:16,287 datashaper.workflow.workflow INFO executing verb filter
14:01:16,331 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
14:01:16,807 graphrag.index.run INFO Running workflow: create_final_nodes...
14:01:16,822 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
14:01:16,823 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
14:01:16,895 datashaper.workflow.workflow INFO executing verb layout_graph
14:01:17,438 datashaper.workflow.workflow INFO executing verb unpack_graph
14:01:17,598 datashaper.workflow.workflow INFO executing verb unpack_graph
14:01:17,904 datashaper.workflow.workflow INFO executing verb filter
14:01:18,62 datashaper.workflow.workflow INFO executing verb drop
14:01:18,91 datashaper.workflow.workflow INFO executing verb select
14:01:18,131 datashaper.workflow.workflow INFO executing verb rename
14:01:18,178 datashaper.workflow.workflow INFO executing verb join
14:01:18,277 datashaper.workflow.workflow INFO executing verb convert
14:01:18,449 datashaper.workflow.workflow INFO executing verb rename
14:01:18,453 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
14:01:18,774 graphrag.index.run INFO Running workflow: create_final_communities...
14:01:18,775 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
14:01:18,776 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
14:01:19,41 datashaper.workflow.workflow INFO executing verb unpack_graph
14:01:19,237 datashaper.workflow.workflow INFO executing verb unpack_graph
14:01:19,402 datashaper.workflow.workflow INFO executing verb aggregate_override
14:01:19,453 datashaper.workflow.workflow INFO executing verb join
14:01:19,535 datashaper.workflow.workflow INFO executing verb join
14:01:19,609 datashaper.workflow.workflow INFO executing verb concat
14:01:19,652 datashaper.workflow.workflow INFO executing verb filter
14:01:20,309 datashaper.workflow.workflow INFO executing verb aggregate_override
14:01:20,719 datashaper.workflow.workflow INFO executing verb join
14:01:20,783 datashaper.workflow.workflow INFO executing verb filter
14:01:20,981 datashaper.workflow.workflow INFO executing verb fill
14:01:21,32 datashaper.workflow.workflow INFO executing verb merge
14:01:21,165 datashaper.workflow.workflow INFO executing verb copy
14:01:21,237 datashaper.workflow.workflow INFO executing verb select
14:01:21,255 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
14:01:21,707 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
14:01:21,708 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
14:01:21,709 graphrag.index.run INFO read table from storage: create_final_entities.parquet
14:01:22,361 datashaper.workflow.workflow INFO executing verb select
14:01:22,475 datashaper.workflow.workflow INFO executing verb unroll
14:01:22,519 datashaper.workflow.workflow INFO executing verb aggregate_override
14:01:22,534 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
14:01:23,22 graphrag.index.run INFO Running workflow: create_final_relationships...
14:01:23,23 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
14:01:23,24 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
14:01:23,30 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
14:01:23,144 datashaper.workflow.workflow INFO executing verb unpack_graph
14:01:23,493 datashaper.workflow.workflow INFO executing verb filter
14:01:23,777 datashaper.workflow.workflow INFO executing verb rename
14:01:23,838 datashaper.workflow.workflow INFO executing verb filter
14:01:24,92 datashaper.workflow.workflow INFO executing verb drop
14:01:24,148 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
14:01:24,233 datashaper.workflow.workflow INFO executing verb convert
14:01:24,414 datashaper.workflow.workflow INFO executing verb convert
14:01:24,419 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
14:01:24,886 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
14:01:24,887 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
14:01:24,888 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
14:01:25,36 datashaper.workflow.workflow INFO executing verb select
14:01:25,218 datashaper.workflow.workflow INFO executing verb unroll
14:01:25,313 datashaper.workflow.workflow INFO executing verb aggregate_override
14:01:25,376 datashaper.workflow.workflow INFO executing verb select
14:01:25,381 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
14:01:25,955 graphrag.index.run INFO Running workflow: create_final_community_reports...
14:01:25,956 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
14:01:25,957 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
14:01:25,968 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
14:01:26,193 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
14:01:26,875 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
14:01:27,486 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
14:01:28,139 datashaper.workflow.workflow INFO executing verb prepare_community_reports
14:01:28,437 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 275
14:01:29,188 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 275
14:01:30,675 datashaper.workflow.workflow INFO executing verb create_community_reports
14:01:49,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:49,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.89099999999962. input_tokens=2069, output_tokens=537
14:01:52,450 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:52,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.89099999999962. input_tokens=2068, output_tokens=561
14:01:52,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:52,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.889999999999418. input_tokens=2050, output_tokens=565
14:01:53,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:53,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.39099999999962. input_tokens=2212, output_tokens=631
14:01:54,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:54,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.82799999999952. input_tokens=2240, output_tokens=596
14:01:54,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:54,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.10899999999856. input_tokens=2178, output_tokens=544
14:01:54,636 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:54,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.235000000000582. input_tokens=2150, output_tokens=611
14:01:54,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:54,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.186999999999898. input_tokens=2071, output_tokens=550
14:01:54,821 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:54,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.34399999999914. input_tokens=2088, output_tokens=600
14:01:55,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:55,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.125. input_tokens=2095, output_tokens=671
14:01:55,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:55,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.610000000000582. input_tokens=2235, output_tokens=626
14:01:55,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:55,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.92199999999866. input_tokens=6195, output_tokens=729
14:01:55,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:55,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.54700000000048. input_tokens=2059, output_tokens=521
14:01:55,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:55,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.734000000000378. input_tokens=2382, output_tokens=673
14:01:56,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:56,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.75. input_tokens=2245, output_tokens=659
14:01:56,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:56,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.515000000001237. input_tokens=2078, output_tokens=683
14:01:56,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:56,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.34400000000096. input_tokens=2703, output_tokens=708
14:01:56,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:56,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.764999999999418. input_tokens=3245, output_tokens=722
14:01:56,808 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:56,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.375. input_tokens=2392, output_tokens=801
14:01:57,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:57,56 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:57,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.389999999999418. input_tokens=2532, output_tokens=769
14:01:57,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.98400000000038. input_tokens=2076, output_tokens=568
14:01:58,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:58,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.73399999999856. input_tokens=2736, output_tokens=759
14:01:58,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:58,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.25. input_tokens=2114, output_tokens=691
14:01:59,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:59,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.311999999999898. input_tokens=2739, output_tokens=809
14:01:59,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:01:59,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.70299999999952. input_tokens=2107, output_tokens=640
14:02:01,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:01,219 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.890999999999622. input_tokens=2481, output_tokens=669
14:02:01,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:01,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.75. input_tokens=2412, output_tokens=740
14:02:03,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:03,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.546999999998661. input_tokens=2391, output_tokens=776
14:02:04,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:04,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.578000000001339. input_tokens=2339, output_tokens=700
14:02:04,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:04,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.51500000000124. input_tokens=2204, output_tokens=691
14:02:04,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:04,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.453000000001339. input_tokens=2344, output_tokens=806
14:02:13,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:13,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.531999999999243. input_tokens=2295, output_tokens=669
14:02:14,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:14,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.485000000000582. input_tokens=2732, output_tokens=895
14:02:14,990 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:14,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.70299999999952. input_tokens=5113, output_tokens=727
14:02:15,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:15,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.078000000001339. input_tokens=2265, output_tokens=568
14:02:15,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:15,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:15,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.17200000000048. input_tokens=3534, output_tokens=775
14:02:15,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.21900000000096. input_tokens=2595, output_tokens=704
14:02:15,758 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:15,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.67200000000048. input_tokens=2437, output_tokens=912
14:02:15,839 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:15,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.593999999999141. input_tokens=2097, output_tokens=599
14:02:15,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:15,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.625. input_tokens=2316, output_tokens=623
14:02:16,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:16,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:16,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.703000000001339. input_tokens=3538, output_tokens=747
14:02:16,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.734000000000378. input_tokens=2998, output_tokens=788
14:02:17,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:17,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.061999999999898. input_tokens=2800, output_tokens=870
14:02:18,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:18,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.281999999999243. input_tokens=2143, output_tokens=567
14:02:18,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:18,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.610000000000582. input_tokens=2084, output_tokens=576
14:02:19,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:19,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.984000000000378. input_tokens=2060, output_tokens=639
14:02:19,694 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:19,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.436999999999898. input_tokens=2204, output_tokens=601
14:02:23,356 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:02:23,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.09400000000096. input_tokens=7976, output_tokens=873
14:02:23,494 datashaper.workflow.workflow INFO executing verb window
14:02:23,498 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
14:02:24,93 graphrag.index.run INFO Running workflow: create_final_text_units...
14:02:24,94 graphrag.index.run INFO dependencies for create_final_text_units: ['create_base_text_units', 'join_text_units_to_relationship_ids', 'join_text_units_to_entity_ids']
14:02:24,95 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
14:02:24,102 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
14:02:24,160 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
14:02:24,401 datashaper.workflow.workflow INFO executing verb select
14:02:24,468 datashaper.workflow.workflow INFO executing verb rename
14:02:24,563 datashaper.workflow.workflow INFO executing verb join
14:02:24,765 datashaper.workflow.workflow INFO executing verb join
14:02:24,843 datashaper.workflow.workflow INFO executing verb aggregate_override
14:02:24,948 datashaper.workflow.workflow INFO executing verb select
14:02:25,75 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
14:02:25,635 graphrag.index.run INFO Running workflow: create_base_documents...
14:02:25,636 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
14:02:25,637 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
14:02:25,809 datashaper.workflow.workflow INFO executing verb unroll
14:02:25,904 datashaper.workflow.workflow INFO executing verb select
14:02:25,972 datashaper.workflow.workflow INFO executing verb rename
14:02:26,54 datashaper.workflow.workflow INFO executing verb join
14:02:26,345 datashaper.workflow.workflow INFO executing verb aggregate_override
14:02:26,452 datashaper.workflow.workflow INFO executing verb join
14:02:26,634 datashaper.workflow.workflow INFO executing verb rename
14:02:26,824 datashaper.workflow.workflow INFO executing verb convert
14:02:27,136 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
14:02:27,582 graphrag.index.run INFO Running workflow: create_final_documents...
14:02:27,582 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
14:02:27,583 graphrag.index.run INFO read table from storage: create_base_documents.parquet
14:02:27,927 datashaper.workflow.workflow INFO executing verb rename
14:02:27,932 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
14:02:28,243 graphrag.index.cli INFO All workflows completed successfully.
